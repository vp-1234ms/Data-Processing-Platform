
command prompt:

Activate virtual environment in vs code

python -m venv venv
venv\Scripts\activate


pip install apache-airflow

pip install -r requirements.txt

docker compose up -d

containers:

                  control center
                   /
                   V

zookeeper  <- broker <- schema registry
                 

Postgres <- web server  <- scheduler


spark master <- spark worker


Cassandra db

pip uninstall kafka-python
pip install confluent-kafka



for /f "tokens=*" %i in ('docker ps -q') do docker stop %I    -> stop all conatiners running 

docker-compose down -v   ->  better use this to stop and remove all those conaitner used 


docker-compose up -d  -> start again


---------------------------------------------------------------
import uuid
from datetime import datetime
from airflow import DAG
from airflow.operators.python import PythonOperator

default_args = {
    'owner': 'airscholar',
    'start_date': datetime(2023, 9, 3, 10, 00)
}

def get_data():
    import requests

    res = requests.get("https://randomuser.me/api/")
    res = res.json()
    res = res['results'][0]

    return res

def format_data(res):
    data = {}
    location = res['location']
    data['id'] = str(uuid.uuid4()) 
    data['first_name'] = res['name']['first']
    data['last_name'] = res['name']['last']
    data['gender'] = res['gender']
    data['address'] = f"{str(location['street']['number'])} {location['street']['name']}, " \
                      f"{location['city']}, {location['state']}, {location['country']}"
    data['post_code'] = location['postcode']
    data['email'] = res['email']
    data['username'] = res['login']['username']
    data['dob'] = res['dob']['date']
    data['registered_date'] = res['registered']['date']
    data['phone'] = res['phone']
    data['picture'] = res['picture']['medium']
    return data

def stream_data():
    import json
    # from kafka import KafkaProducer
    import time
    import logging

    from confluent_kafka import Producer

    # Initialize producer configuration and produce data
    res = get_data()
    res = format_data(res)
    conf = {'bootstrap.servers': 'localhost:9092'}
    producer = Producer(**conf)
    producer.produce('users_created', json.dumps(res).encode('utf-8'))
    producer.flush()

    # res = get_data()
    # res = format_data(res)
    # producer = KafkaProducer(bootstrap_servers=['localhost:9092'], max_block_ms=5000)
    # producer.send('users_created', json.dumps(res).encode('utf-8'))
    # producer = KafkaProducer(bootstrap_servers=['broker:29092'], max_block_ms=5000)
#     curr_time = time.time()

#     while True:
#         if time.time() > curr_time + 60: #1 minute
#             break
#         try:
#             res = get_data()
#             res = format_data(res)

#             producer.send('users_created', json.dumps(res).encode('utf-8'))
#         except Exception as e:
#             logging.error(f'An error occured: {e}')
#             continue

# with DAG('user_automation',
#          default_args=default_args,
#          schedule_interval='@daily',
#          catchup=False) as dag:

#     streaming_task = PythonOperator(
#         task_id='stream_data_from_api',
#         python_callable=stream_data
#     )

stream_data()

pip install confluent-kafka



python dags\kafka_stream.py  -> to create borker name "user_created"

python dags\kafka_stream.py  -> to send data first time

python dags\kafka_stream.py   -> to send data second time



---------------------------------------------------------------------------



Go to power shell: (reason why airflow is not running on 8080 port)

cd C:\Users\vaibh\Downloads\e2e-data-engineering\script


Get-Acl .\entrypoint.sh


----------------------------------------------------------------------------------

Now go to 
Open entrypoint.sh in VS Code.
At the bottom-right corner, you'll see the current line ending format (e.g., CRLF).
Click on CRLF, select LF to convert it to Unix-style line endings.
Save the file.

-------------------------------------------------------------------------------

START SCHEDULER MANUALLY IN DOCKER IF NOT STARTED:::

docker-compose restart kafka e2e-data-engineering-webserver-1  -> NEED TO RESTART KAFKA AND WEB SERVER


-------------------------------------------------------------------------------------

AIRFLOW WEB SERVER: USER ID AND PASSWORD IS -> admin, admin

 

RUN DAG:

------------------------------------------------------------
import uuid
from datetime import datetime
from airflow import DAG
from airflow.operators.python import PythonOperator

default_args = {
    'owner': 'airscholar',
    'start_date': datetime(2023, 9, 3, 10, 00)
}

def get_data():
    import requests

    res = requests.get("https://randomuser.me/api/")
    res = res.json()
    res = res['results'][0]

    return res

def format_data(res):
    data = {}
    location = res['location']
    data['id'] = str(uuid.uuid4()) 
    data['first_name'] = res['name']['first']
    data['last_name'] = res['name']['last']
    data['gender'] = res['gender']
    data['address'] = f"{str(location['street']['number'])} {location['street']['name']}, " \
                      f"{location['city']}, {location['state']}, {location['country']}"
    data['post_code'] = location['postcode']
    data['email'] = res['email']
    data['username'] = res['login']['username']
    data['dob'] = res['dob']['date']
    data['registered_date'] = res['registered']['date']
    data['phone'] = res['phone']
    data['picture'] = res['picture']['medium']
    return data

def stream_data():
    import json
    # from kafka import KafkaProducer
    import time
    import logging

    from confluent_kafka import Producer

    conf = {'bootstrap.servers': 'broker:29092'}
    producer = Producer(**conf)
    curr_time = time.time()

    while True:
        if time.time() > curr_time + 60:  # 1 minute
            break
        try:
            res = get_data()
            res = format_data(res)

            producer.produce('users_created', json.dumps(res).encode('utf-8'))
        except Exception as e:
            logging.error(f'An error occurred: {e}')
            continue

    producer.flush()  # Make sure all messages are sent

with DAG('user_automation',
         default_args=default_args,
         schedule_interval='@daily',
         catchup=False) as dag:

    streaming_task = PythonOperator(
        task_id='stream_data_from_api',
        python_callable=stream_data
    )



RUN:

python dags\kafka_stream.py 

AND NOW REFRESH AIRFLOW UI WILL SEE DAG CTREATED AND CLICK TRIGGER TO START THE PROCESS
------------------------------------------------------------




SO NOW WE HAVE EXTRACTED OR INGESTED DATA FROM API AND USING APACHE AIRFLOW TRIGGERED PUT IT INTO BROKER OF APACHE KAFKA 

--------------------------------------------------------------
NOW COLLECTING IT IN USING APACHE CASSANDRA

SO WORKING ON spark_stream.py
RUN: 

pip install cassandra-driver

pip install spark pyspark




